{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Homework 3, Group 23 - version 6</H1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Importing libraries </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/jagg/Data/airbnb_data\"\n",
    "#file_path = \"airbnb_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gvf import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Read .csv file </H3>\n",
    "\n",
    "<p> Using pandas to read the .csv file and visualizing it on the Data Frame </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Create .tsv files </H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "To create each .tsv file, we iterate the .csv source file, line by line, storing each line of the information in variables, one  for each field. Once the end of the row is reached, we join the information into a new file where the data will be separated by tabulations.  The process will be repeated until we reach the last row of the source document.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new file with tsv extension for each row of the dataset.\n",
    "for i in range(0, len(df)):\n",
    "\n",
    "    # Open .tsv file.\n",
    "    file = open(file_path + \"/doc_\" + str(i) + \".tsv\", \"w\", encoding = \"utf-8\")\n",
    "        \n",
    "    # Get data from dataframe and saving it into a variable\n",
    "    average_rate_per_night = str(df[\"average_rate_per_night\"][i])\n",
    "    bedrooms_count = str(df[\"bedrooms_count\"][i])\n",
    "    city = str(df[\"city\"][i])\n",
    "    date_of_listing = str(df[\"date_of_listing\"][i])    \n",
    "    description = str(df[\"description\"][i])\n",
    "    latitude = str(df[\"latitude\"][i])\n",
    "    longitude = str(df[\"longitude\"][i])\n",
    "    title = str(df[\"title\"][i])\n",
    "    url = str(df[\"url\"][i])\n",
    "                \n",
    "    # Join the fields that will be written in the file\n",
    "    entry = \"\t\".join([average_rate_per_night, \n",
    "                        bedrooms_count,\n",
    "                        city, \n",
    "                        date_of_listing, \n",
    "                        description, \n",
    "                        latitude, \n",
    "                        longitude,\n",
    "                        title,\n",
    "                        url\n",
    "                       ])\n",
    "    \n",
    "    \n",
    "    # Write in the .tsv file\n",
    "    file.write(entry)\n",
    "    \n",
    "    # close the .tsv file\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    We now have a file with .tsv extension for each line of the original .csv document stored in the folder \"lairbnb_data\".\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Modify .tsv files </H3>\n",
    "\n",
    "<p> We are now going to modify the information stored on <b><u>description</u></b> and <b><u>title</u></b> columns respectively.</p>\n",
    "<p> Once the files are created, line breaks, punctuation marks and unwanted characters are eliminated.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "\n",
    "    file = open(file_path + \"/doc_\" + str(i) + \".tsv\", \"r\",\n",
    "                encoding = \"utf-8\")                                        # Only read \n",
    "    entry = file.read()                                                    # Storing the information\n",
    "    file.close()                                                           # Closing the file\n",
    "    \n",
    "    entry = entry.split(\"\\t\")                                              # Separating with tabs\n",
    "\n",
    "    description = entry[4]                                                 # Accesing the description info\n",
    "    description = text_formatting(description)                             # Sending description info to the function\n",
    "    add_voc(description)                                                   # Sending description info to the Vocabulary\n",
    "    entry[4] = description                                                 # Receiving the description info modified\n",
    "\n",
    "    title = entry[7]                                                       # Accesing the title info\n",
    "    title = text_formatting(title)                                         # Sending title info to the function\n",
    "    add_voc(title)                                                         # Sending title info to the Vocabulary\n",
    "    entry[7] = title                                                       # Receiving de title info modified\n",
    "\n",
    "    entry = \"\t\".join(entry)                                              # Joining the info to put it back in file\n",
    "\n",
    "    file = open(file_path + \"/doc_\" + str(i) + \".tsv\", \"w\", \n",
    "                encoding = \"utf-8\")                                        # Modifying the file with the processed data\n",
    "    file.write(entry)                                                      # Writing in the file\n",
    "    file.close()                                                           # Closing the file\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Build vocabulary </H3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting and deleting duplicate words\n",
    "vocabulary = list(sorted(set(vocabulary)))    # The vocabulary variable was created in the in the initialization section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step we will go through the vocabulary to assign an index to each word.\n",
    "# At the same time we put them in lower case.\n",
    "\n",
    "file = open(\"vocabulary.txt\", \"w\", encoding = \"utf-8\")            \n",
    "\n",
    "for i in range(len(vocabulary)):            \n",
    "    file.write(str(i) + \": \" + vocabulary[i].lower() + \"\\n\")\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we do in this step ?\n",
    "vocabulary_file = open(\"vocabulary.txt\", \"r\", encoding = \"utf-8\")\n",
    "vocabulary = vocabulary_file.read()\n",
    "vocabulary = vocabulary.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Dictionary Creation</H3>\n",
    "<p>\n",
    "    Going through each row of the Data Frame is like inspecting each created tsv file.<br>\n",
    "If we look for each words of the vocabulary in the columns description and title of the data frame, we will know in how many files each word appears.<br>\n",
    "Then we will use the index of the vocabulary as index of the dictionary and the index numbers of each row, as the document number for the dictionary.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_index in range(len(df)):\n",
    "\n",
    "    # Open .tsv file.\n",
    "    file = open(file_path + \"/doc_\" + str(doc_index) + \".tsv\", \"r\", encoding = \"utf-8\")\n",
    "    \n",
    "    # read entry\n",
    "    entry = file.read()\n",
    "    entry = entry.split(\"\\t\")\n",
    "    \n",
    "    # get description and title\n",
    "    description = entry[4]\n",
    "    title = entry[7]\n",
    "    \n",
    "    # merge in a single string variable the title and the description\n",
    "    des_tit = description + \" \" + title\n",
    "    \n",
    "    des_tit = set(des_tit.split(\" \"))\n",
    "    \n",
    "    # for every word in the description and title\n",
    "    for word in des_tit:\n",
    "        \n",
    "        # for every line in the vocabulary\n",
    "        for i in range(1, len(vocabulary)-1):\n",
    "            \n",
    "            # get the vocabulary index and word form the respective line\n",
    "            term_id = vocabulary[i].split(\" \")[0].replace(\":\", \"\")\n",
    "            term = vocabulary[i].split(\" \")[1]\n",
    "                   \n",
    "            # if it's not yet in the dictionary\n",
    "            if (word == term) and (term_id not in dictionary):\n",
    "                dictionary[term_id] = [doc_index]                    # add the term_id and doc index to the dictionary\n",
    "                break\n",
    "                \n",
    "            elif (word == term) and (term_id in dictionary):       # else if it's already in the dictionary\n",
    "                dictionary[term_id].append(doc_index)                   # append the doc index to the dictionary\n",
    "                break\n",
    "    file.close()\n",
    "    \n",
    "vocabulary_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Visualizing the Dictionary</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>3.1) Conjunctive query</H2>\n",
    "<p>Since we are dealing with conjunctive queries (AND), each of the returned documents should contain all the words in the query. The final output of the query must return, if present, the following information for each of the selected documents: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = str(input())\n",
    "query_copy = query.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST QUERY\n",
    "\n",
    "query = text_formatting(query)\n",
    "query = query.split(\" \")\n",
    "query_copy = query_copy.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> \n",
    "    In this code we implement the query search, using two list, one for the word's index and other the document's index, we found and return the intersection between them.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_file = open(\"vocabulary.txt\", \"r\", \n",
    "                       encoding = \"utf-8\")                     # Open the vocabulary file\n",
    "vocabulary = vocabulary_file.read()                            # Reading \n",
    "vocabulary = vocabulary.split(\"\\n\")                            # Spliting each word in a new Row\n",
    "\n",
    "term_id_list = list()                                          # Creating a List for Word's index\n",
    "doc_list = list()                                              # Creating a List of matching documents\n",
    "\n",
    "for word in query:                                             # Searching for each word in the query into the vocabulary  \n",
    "        \n",
    "    boolean = False\n",
    "    \n",
    "    # for every line in the vocabulary\n",
    "    for i in range(1, len(vocabulary)-1):\n",
    "\n",
    "        # get the vocabulary index and word form the line\n",
    "        term_id = vocabulary[i].split(\" \")[0].replace(\":\", \"\")\n",
    "        term = vocabulary[i].split(\" \")[1]\n",
    "\n",
    "        if (word == term):\n",
    "            boolean = True\n",
    "            term_id_list.append(term_id)\n",
    "            break\n",
    "    \n",
    "    if boolean == False:\n",
    "        term_id_list = list()\n",
    "        break\n",
    "                \n",
    "vocabulary_file.close()\n",
    "\n",
    "## print(term_id_list)\n",
    "\n",
    "for term_id in term_id_list:\n",
    "    if term_id in dictionary:\n",
    "        doc_list.append(dictionary[term_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Presentation of the query result<br>\n",
    "    Documents that meet the selected criteria \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(doc_list) > 0:\n",
    "    \n",
    "    selected_docs = set(doc_list[0])\n",
    "\n",
    "    for l in doc_list:\n",
    "        selected_docs = selected_docs.intersection(set(l))\n",
    "else:\n",
    "    selected_docs = list()\n",
    "    \n",
    "selected_docs = list(selected_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[selected_docs].drop(labels = [\"Unnamed: 0\", \n",
    "                                      \"average_rate_per_night\", \n",
    "                                      \"bedrooms_count\", \n",
    "                                      \"date_of_listing\", \n",
    "                                      \"latitude\", \n",
    "                                      \"longitude\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Compute TFIDF and Cosine Similarity </H3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general principle is to compute the distance between the query and each one of the documents. This is done by creating a vector for each one of them. Each component of the vectors corresponds to a word. In the query vector, if the word is contained, the component will be $1$, otherwise it will be $0$. In the documents vectors, the components are the $TFIDF = TF \\cdot IDF$ of each word in the given document. The $TF$ (term frequency) is defined as $TF = \\frac{\\textrm{number of occurences of a word}}{\\textrm{total number of words}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new dictionary\n",
    "dictionary2 = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_file = open(\"vocabulary.txt\", \"r\", \n",
    "                       encoding = \"utf-8\")                     # Open the vocabulary file\n",
    "vocabulary = vocabulary_file.read()                            # Reading \n",
    "vocabulary = vocabulary.split(\"\\n\")\n",
    "\n",
    "for i in range(1, len(dictionary)+1):\n",
    "\n",
    "    # get the vocabulary index and word form the line\n",
    "    term_id = vocabulary[i].split(\" \")[0].replace(\":\", \"\")\n",
    "    term = vocabulary[i].split(\" \")[1]\n",
    "    \n",
    "    # for every document\n",
    "    for doc_index in range(len(df)):\n",
    "        \n",
    "        # open file\n",
    "        file = open(file_path + \"/doc_\" + str(doc_index) + \".tsv\", \"r\", \n",
    "                    encoding = \"utf-8\")\n",
    "        \n",
    "        # read entry\n",
    "        entry = file.read()\n",
    "        entry = entry.split(\"\\t\")\n",
    "\n",
    "        # get description and title\n",
    "        description = entry[4]\n",
    "        title = entry[7]\n",
    "\n",
    "        # merge in a single string variable the title and the description\n",
    "        des_tit = description + \" \" + title\n",
    "        des_tit = des_tit.split(\" \")\n",
    "        \n",
    "        # compute tfidf\n",
    "        tfidf = TFIDF(term_id, term, des_tit)\n",
    "        \n",
    "        # if it's not yet in the dictionary\n",
    "        if (term_id not in dictionary2):\n",
    "            dictionary2[term_id] = [(doc_index, tfidf)]      # add the term_id and (doc index, tfidf) tuple to the dictionary\n",
    "        elif (term_id in dictionary2):                       # else if it's already in the dictionary\n",
    "            dictionary2[term_id].append((doc_index, tfidf))  # append the (doc index, tfidf) tuple\n",
    "        \n",
    "        # close file\n",
    "        file.close()\n",
    "        \n",
    "# close vocabulary\n",
    "vocabulary_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_file = open(\"vocabulary.txt\", \"r\", \n",
    "                       encoding = \"utf-8\")                     # Open the vocabulary file\n",
    "vocabulary = vocabulary_file.read()                            # Reading \n",
    "vocabulary = vocabulary.split(\"\\n\")\n",
    "\n",
    "v_query = [0]*(len(dictionary)+1)\n",
    "v_docs = list()\n",
    "\n",
    "for doc_index in range(len(df)):\n",
    "    v_docs.append([])\n",
    "    for j in range(len(dictionary)+1):\n",
    "        v_docs[doc_index].append(0)\n",
    "\n",
    "\n",
    "for i in range(1, len(dictionary)+1):\n",
    "\n",
    "    # Get the vocabulary index and word form the line\n",
    "    term_id = vocabulary[i].split(\" \")[0].replace(\":\", \"\")\n",
    "    term = vocabulary[i].split(\" \")[1]\n",
    "\n",
    "    for word in query:\n",
    "        if (word == term):\n",
    "            v_query[int(term_id)] = 1\n",
    "    \n",
    "    for doc_index in range(len(df)):\n",
    "        \n",
    "        v_docs[doc_index][int(term_id)] = dictionary2[term_id][doc_index][1]\n",
    "\n",
    "vocabulary_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert query vector to numpy array\n",
    "a_query = np.array(v_query)\n",
    "\n",
    "# Create a heap of scores and a dictionary of scores\n",
    "heap = list()\n",
    "heapq.heapify(heap)\n",
    "scores_dictionary = dict()\n",
    "\n",
    "# For every document\n",
    "\n",
    "for doc_index in range(len(df)):\n",
    "    \n",
    "    # Convert document vector to numpy array\n",
    "    a_doc = np.array(v_docs[doc_index])\n",
    "    \n",
    "    # Compute the cosine between the query vector and the document vector\n",
    "    cos = np.dot(a_query, a_doc)/(np.linalg.norm(a_query)*np.linalg.norm(a_doc))\n",
    "    \n",
    "    # Put the result in the dictionary\n",
    "    scores_dictionary[doc_index] = cos\n",
    "    heapq.heappush(heap, cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number k of top documents\n",
    "k = 10\n",
    "\n",
    "# Get the ordered list of top_k scores from the heap\n",
    "top_k = heapq.nlargest(k, heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_docs = list()\n",
    "\n",
    "# Replace scores in the heap with corresponding documents\n",
    "for i in range(len(top_k)):\n",
    "    top_k_docs.append(list(scores_dictionary.keys())[list(scores_dictionary.values()).index(top_k[i])])\n",
    "    del scores_dictionary[list(scores_dictionary.keys())[list(scores_dictionary.values()).index(top_k[i])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df.iloc[top_k_docs].drop(labels = [\"Unnamed: 0\", \n",
    "                                           \"average_rate_per_night\", \n",
    "                                           \"bedrooms_count\", \n",
    "                                           \"date_of_listing\", \n",
    "                                           \"latitude\", \n",
    "                                           \"longitude\"], axis = 1)\n",
    "df_results.index = list(range(1, k+1))\n",
    "\n",
    "df_results[\"scores\"] = [round(x,2) for x in top_k]\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<H3>Step 4: Define a new score!</H3>\n",
    "<p>The general idea is to create a ***scoring function*** and assign a score to each document. The scoring function we chose to use is a weighted sum of three scores, associated to ***distance***, ***number of bedrooms***, and ***price*** (average rate per night).</p>  \n",
    "\n",
    "<p>The weights have been estimated heuristically, assuming that the distance has a greater impact on the quality of the results than any other parameter, while the contribution of the number of bedrooms and price is almost identical. We therefore chose to assign a weight $w_d = 0.6$ to the distance, $w_b = 0.2$ to the number of bedrooms and $w_p = 0.2$ to the price.</p>\n",
    "\n",
    "<p>The scoring function for the single variables are a ***negative exponential*** $y = e^{-x/10}$ for the distance (taking the value ***1***, maximum score, when the distance is equal zero, and so the city is exactly the one we were searching for), and a ***gaussian*** $y = e^{-(x-x_{ex})^2}$ for the number of bedrooms and the price (which have a peak value ***1*** corresponding to the exact number of bedrooms or th exact price, while the score decreases if the variables are too high or too low with respect to the exact values searched by the user).</p>\n",
    "\n",
    "<p>The final scoring function is thus:<br>\n",
    "$y = w_d \\cdot e^{-x/10} + w_b \\cdot e^{-(n-n_{ex})^2} + w_p \\cdot e^{-(p-p_{ex})^2}$\n",
    "</p>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = str(input())\n",
    "query_copy = query.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = text_formatting(query)\n",
    "query = query.split(\" \")\n",
    "query_copy = query_copy.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary3 = dict()\n",
    "cities_dictionary = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_index in range(len(df)):\n",
    "\n",
    "    # Open .tsv file.\n",
    "    file = open(file_path + \"/doc_\" + str(doc_index) + \".tsv\", \"r\", encoding = \"utf-8\")\n",
    "    \n",
    "    # read entry\n",
    "    entry = file.read()\n",
    "    entry = entry.split(\"\\t\")\n",
    "    \n",
    "    # get description and title\n",
    "    average_price_per_night = entry[0]\n",
    "    bedrooms_count = entry[1]\n",
    "    city = entry[2]\n",
    "    latitude = entry[5]\n",
    "    longitude = entry[6]\n",
    "    score = 0\n",
    "    distance = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    # update dictionary\n",
    "    dictionary3[doc_index] = [average_price_per_night, \n",
    "                              bedrooms_count, \n",
    "                              city, \n",
    "                              latitude, \n",
    "                              longitude, \n",
    "                              distance,\n",
    "                              score]\n",
    "    \n",
    "    city = city.lower()\n",
    "    if city not in cities_dictionary:\n",
    "        cities_dictionary[city] = (float(latitude), float(longitude))\n",
    "    \n",
    "    # Close .tsv file\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_price = 0\n",
    "q_beds = 0\n",
    "q_coord = (0,0)\n",
    "\n",
    "# In the query\n",
    "\n",
    "for i in range(len(query)):\n",
    "    \n",
    "    # Get average price per night \n",
    "    if \"$\" in query[i]:\n",
    "        q_price = float(query[i].replace(\"$\", \"\"))\n",
    "        \n",
    "    # Get bedrooms count\n",
    "    if query[i] == \"bedroom\":\n",
    "        q_beds = int(query[i-1])\n",
    "        \n",
    "# Get city coordinates        \n",
    "for city in cities_dictionary:\n",
    "    \n",
    "    city = city.split(\" \")\n",
    "    \n",
    "    if len(city) == 1:\n",
    "        for word in query_copy:\n",
    "            if word == city[0]:\n",
    "                city = city[0]\n",
    "            \n",
    "                q_coord = cities_dictionary[city]\n",
    "        \n",
    "    else:\n",
    "        for i in range(len(query_copy)):\n",
    "            if (query_copy[i] == city[0]) and (query_copy[i+1] == city[1]):\n",
    "                city = city[0] + \" \" + city[1]\n",
    "            \n",
    "                q_coord = cities_dictionary[city]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if q_coord != (0,0):\n",
    "    w_city = 0.6\n",
    "else:\n",
    "    w_city = 0\n",
    "    \n",
    "if q_beds != 0:\n",
    "    w_beds = 0.2\n",
    "else:\n",
    "    w_beds = 0\n",
    "    \n",
    "if q_price != 0:\n",
    "    w_price = 0.2\n",
    "else:\n",
    "    w_price = 0\n",
    "    \n",
    "heap = list()\n",
    "heapq.heapify(heap)\n",
    "            \n",
    "for doc_index in range(len(df)):\n",
    "    \n",
    "    d_price = dictionary3[doc_index][0]\n",
    "    d_price = d_price.replace(\"$\", \"\")\n",
    "    d_price = float(d_price)\n",
    "    \n",
    "    try:\n",
    "        d_beds = int(dictionary3[doc_index][1])\n",
    "    except ValueError:\n",
    "        d_beds = 0\n",
    "        \n",
    "    d_city = dictionary3[doc_index][2]\n",
    "    d_city = d_city.lower()\n",
    "    d_coord = cities_dictionary[d_city]\n",
    "\n",
    "    dist = geodesic(q_coord, d_coord).km\n",
    "             \n",
    "    s_city = math.exp(-(dist/10))\n",
    "    s_beds = math.exp(-(d_beds - q_beds)**2)\n",
    "    s_price = math.exp(-(d_price - q_price)**2)\n",
    "    \n",
    "    score = w_city*s_city + w_beds*s_beds + w_price*s_price\n",
    "    \n",
    "    dictionary3[doc_index][5] = dist\n",
    "    dictionary3[doc_index][6] = score\n",
    "    \n",
    "    dictionary3[doc_index] = score\n",
    "    \n",
    "    heapq.heappush(heap, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number k of top documents\n",
    "k = 10\n",
    "\n",
    "# Get the ordered list of top_k scores from the heap\n",
    "top_k = heapq.nlargest(k, heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_docs = list()\n",
    "\n",
    "# Replace scores in the heap with corresponding documents\n",
    "for i in range(len(top_k)):\n",
    "    doc_index = list(dictionary3.keys())[list(dictionary3.values()).index(top_k[i])]\n",
    "    top_k_docs.append(doc_index)\n",
    "    del dictionary3[doc_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df.iloc[top_k_docs].drop(labels = [\"Unnamed: 0\", \n",
    "                                           \"average_rate_per_night\", \n",
    "                                           \"bedrooms_count\", \n",
    "                                           \"date_of_listing\", \n",
    "                                           \"latitude\", \n",
    "                                           \"longitude\"], axis = 1)\n",
    "df_results.index = list(range(1, k+1))\n",
    "df_results.index.name = 'ranking'\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
